# 1. Hive

1. [Hive Overview](#1-Hive-Overview)
2. [Hive Architecture](#2-Hive-Architecture)
3. [Hive Client](#3-Hive-Client)
4. [Hive Table의 특징](#4-Hive-Table의-특징)
5. [HiveQL](#5-HiveQL)

---

## 1. Hive Overview

### 1-1. Hive란?
- Hive 는 HDFS(또는 다른 분산 스토리지)의 대량의 데이터를 SQL로 다룰 수 있도록 하는 도구(소프트웨어 패키지)이다.
- 오픈소스이고 무료로 사용할 수 있다.

### 1-2. Hive의 장점
1. HDFS 를 대상으로 SQL을 사용할 수 있다.
2. 다양한 client 프로토콜을 지원한다. (ODBC, JDBC, Thrift)
3. 대상 파일의 위치로 다양한 외부 저장소를 선택할 수 있다. (HDFS, S3, Azure, storage)
4. 실행 엔진(execution engine)을 선택할 수 있다.
5. 현재 다양한 언어의 클라이언트 라이브러리가 나와있다. (Java, Python, C++, Ruby, Javascript 등)

### 1-3. Hive의 단점
1. 성능이 Execution Engine 의 종류, 동작방식, 상태에 따라 정해진다.
2. Transaction을 지원하지 않는다. (4부터는 지원한다고는 하지만, 아직 4를 쓰는 패키지가 거의 없다.)
3. 쿼리가 (상대적으로) 느리기 때문에 실시간에 적합하지 않다.

---

## 2. Hive Architecture

![image](https://github.com/seonwook97/Data-Engineering/assets/92377162/6897d179-fe9f-4b98-9ec4-aacc1939c832)

### 2-1. Hive Client
- Python, Java, C++, Ruby 등 다양한 언어로 만들어진 Hive client driver 는 JDBC, ODBC, Thrift driver를 이용해서 hive server 에 쿼리할 수 있다.
  - Thrift 는 hive server 1

### 2-2. Hive Service
- Hive service 는 client 로부터 온 요청을 처리한다. 다양한 구성요소로 구성되어있다.

#### 2-2-1. Hive Server 2
- Client의 query를 받아주는 서버이다. JDBC, ODBC, Thrift 프로토콜로 온 요청을 처리할 수 있다. Hive server 1 에서는 하나의 요청밖에 처리하지 못했지만, Hive server 2에서는 동시에 여러 요청을 처리할 수 있게되었다.

#### 2-2-2. Hive Driver
- Driver에서 HiveQL을 받아준다. 또한 user의 세션을 관리한다.

#### 2-2-3. Metastore
- 테이블, 파티션, 컬럼, HDFS 파일 위치, ser/de 등의 테이블을 읽기 위한 메타데이터를 관리한다.
- 메타스토어는 내부에 구성할 수도 있고, 외부의 데이터베이스(RDBMS 종류)를 이용할 수도 있다. 클러스터가 HA구성되어 있거나, 데이터나 유저의 양이 많다면 외부 데이터베이스를 메타스토어로 구성하는 것이 좋다.

#### 2-2-4. Compiler(paser)
- Driver로 온 요청에 대해 parsing, semantic analysis, type check 등을 한다.
- 문제가 없다면 실행 계획(execution plan)을 세운다.
- 실행 계획은 DAG(directed acyclic graph)형태가 된다.
  - DAG 의 각 step 은 map 또는 reduce 작업에 매핑된다.

#### 2-2-5. Execution Engine
- compiler의 결과에 다라서 execution plan 을 진행한다. execution engine 은 선택할 수 있다.
- 기본은 mapreduce 이다.

#### 2-2-6. Beeline
- JDBC를 이용해서 바로 hive driver 로 연결하는 커맨드라인 클라이언트이다.

### 2-3. Processing and Resource Management
- Hive 는 실행 계획으로 구성된 DAG를 수행할 engine을 선택할 수 있다.
- 기본으로 MapReduce 프레임워크가 동작한다. 또 다른 쓸만한 선택지로는 Apache Tez 가 있다.

### 2-4. Distributed Storage
- 결국 Hive 가 바라보는 파일은 분산 스토리지이다.
- 쿼리의 실행 전, 과정, 결과로 분산 파일 시스템을 조작하게 된다.

---

## 3. Hive Client
- Hive를 이용할 수 있는 방법은 많다. 기본 hive shell 에 들어가도 되고, JDBC driver 를 이용해서 SQL Client 도구들을 이용할 수도 있다. (Workbench, Datagrip 등)
- 실습에서 사용하는 hive server 버전은 `Hive 3.1.3-amzn-1` 이다.
- 실습에서는 EMR Primary node에서 hive beeline shell을 이용한다.

### 3-1. Hive Shell
- EMR primary node 에 접속해서 hive 명령어를 치면 hive shell로 접속할 수 있다.
- hive shell 은 old version 이므로 권장하지 않는다.

### 3-2. Hive beeline
- 새로운 버전의 hive shell이다. EMR Primary node에서 명령어로 접속할 수 있다.

  ```Shell
  beeline -u jdbc:hive2://localhost:10000/default -n hive
  ```
  - n으로 username을 준다.
  - 비밀번호가 있는 경우, -p 옵션으로 준다.
  - EMR primary 에서 접속하는 경우, -n hdfs 유저로 접근하면 hive 로 모든경로에 read/write 가 가능하다.

- 포트포워딩을 포트가 10000 이므로 port forwarding 을 한다면 local에서도 접속할 수 있다.
  - 단, 서버와 맞는 버전의 hive beenline 실행파일이 있어야한다.

    ```Shell
    beeline -u jdbc:hive2://localhost:10000/default -n hadoop@ec2-xxx-xxx-xxx-xxx.us-west-2.compute.amazonaws.com -d org.apache.hive.jdbc.HiveDriver
    ```

### 3-3. Workbench 설치
- workbench 는 무료 도구이다. EMR을 workbench 로 이용하기 
  - 가이드 : https://docs.aws.amazon.com/emr/latest/ReleaseGuide/HiveJDBCDriver.html
- 다운로드링크 에서 다운로드를 받고,
  - 다운로드 : https://www.sql-workbench.eu/downloads.html
- 설치가이드에 따라서 windows/mac/linux 각 환경에 맞게 설치한다.
  - 설치가이드 : https://www.sql-workbench.eu/manual/install.html
  - JRE 를 설치하는 스크립트를 실행
  - 프로그램을 실행하는 스크립트 실행
- Amazon JDBC 드라이버를 다운로드 받는다.
  - Amazon JDBC 드라이버 다운로드 : http://awssupportdatasvcs.com/bootstrap-actions/Simba/
  - Hive JDBC 실습 버전 2.6.17.1020
- 프로그램을 실행하고, 새로운 driver를 추가, 다운로드받은 Amazon Hive JDBC jar 를 선택한다.
- port forwarding 으로 EMR Primiary node 의 hive port(10000)로 localhost에서 접속할 수 있도록 한다.
- 새로운 connection 에 jdbc:hive2://localhost:10000/default 로 접속한다.

---

## 4. Hive Table의 특징

### 4-1. internal/external/temporary table

#### 4-1-1. internal table
- Hive 가 자신의 데이터를 소유하는 형태를 말한다.
- hive 에대해서 external table 이라는 선언으로 create table 을 하지 않는다면, 기본으로 적용된다.
- hive 의 전용 경로 하위에 자동으로 생성된다. location 옵션으로 따로지정할 수도 있다.
- hive의 table 이나 partition을 drop 하면, 해당 경로의 HDFS 파일 또한 지워진다.

#### 4-1-2. external table
- 선언시 `create external table if not exists [external-table-name]...` 로 선언한다.
- external table 은 원본 데이터를 hive 가 관리하지 않는다.
- Hive 의 영역 외에도 테이블을 사용하고 싶을 때 사용한다. ( 거의 모든 경우에 사용한다.)
- 데이터가 같은 HDFS의 경로 뿐만 아니라 외부의 경로로도 지정할 수 있다.
  - Federation 된 다른 하둡 클러스터
  - Webhdfs 로 접근가능한 다른 클러스터의 경로
  - Amazon S3, Azure storage 등의 클라우드 스토리지
- external table 의 partition이나 table 을 drop 하면, hive 상의 메타데이터에서만 삭제되고 원본데이터에는 변화가 없다.

#### 4-1-3. external table 의 단점
- 데이터를 로드하는 프로그램이 hive 를 통하지 않고 파일을 저장했다면, hive가 해당 파일의 정보를 알 수 있도록 메타정보를 업데이트 해줘야 한다. 이 작업을 따로 수행해줘야 hive의 쿼리 결과가 새로운 파일을 인식할 수 있다.
- 주로 파티션 단위로 데이터를 추가, 삭제 하고 메타데이터를 업데이트 한다.
- 새롭게 데이터가 추가되거나 삭제된 경우, alter table로 할 수 있다.
  
  ```HiveQL
  alter table table_name add partition (partition1=x, partition2=2, ..)
  ```
  ```HiveQL
  alter table table_name drop partition (partition1=x, ...)
  ```

- 또한 msck 로도 한 번에 메타데이터 업데이트가 가능하다.
  
  ```HiveQL
  MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];
  ```
    - 단, 데이터가 많은 경우 MSCK 가 오래걸려서 timeout 에 걸리거나 OOM이 날 수 있다.
    - **hive.msck.repair.batch.size** 로 내부적인 배치사이즈를 조정해서 OOM을 방지할 수 있다.

#### 4-1-4. temporary table
- 해당 hive session 에서만 유지되는 테이블이다.
- 보통 select 의 결과를 임시테이블로 만들어서, 새로운 쿼리에서 참조할 때 쓰려고 사용한다.

#### 4.1.5 command
- 다음 명령어로 internal/external 을 구분할 수 있다.

  ```HiveQL
  DESCRIBE FORMATTED table_name;
  show create table table_name;
  ```

### 4-2. Format
- Hive 를 이용한 아키텍처에서 주로 데이터를 저장하는 쪽은 Hive 를 통하지 않고, 다이렉트로 파일로 쓰는 경우가 많다. 데이터를 쓰기 전에 스키마만 하이브를 통해서 조회하는 식이다. (external table)
- 즉 Hive가 데이터를 저장하려면 저장하는 쪽에서 어떤 형식으로 저장했는지를 알아야, 테이블 정의 스키마에 맞기 파일을 읽을 수 있다. 따라서 Hive의 테이블 속성에는 무조건 데이터를 어떤 형식으로 다루어야 하는지 정보가 필요하다.

#### 4-2-1. Hadoop 에서 자주 사용하는 데이터 포맷
- Hadoop에서 자주 사용하는 데이터 포맷은 공통적으로 다음 특징을 가진다.
  - 기계가 읽을 수 있는 형식이다.(human readable X)
  - 파일이 여러 디스크에 나눠져 있을 수 있어서 확장 가능한 형식을 가지는 포맷이다.
  - 파일 자체가 형식을 가지고 있다.
  - 대량의 데이터를 읽는데 부하가 적다.
- 일반 프로그래밍을 할 때 자주 사용하는 csv, xml, json 등은 확장 가능한 포맷이 아니다.

#### 4-2-2. Avro
- row 기반의 바이너리 storage 포맷이다. 데이터의 스키마의 정의는 json 으로 되어있다.
- 데이터 serialization capacity 가 좋다.
- schema evolution 이 가능하고, 유연하다.

#### 4-2-3. Parquet
- big-data에 이상적인 columnar storage 이다.
  - 데이터 필터링(map), aggregation(reduce) 의 작업에서 빠르다.
  - 압축 효율이 좋다.
- 복잡한 nested data 구조를 표현할 수 있는 columnar 포맷이다.

#### 4-2-4. ORC
- Optimized Row Columnar (ORC) file format을 지향하고, 기본적으로 columnar의 특징을 갖는다.
- 파일 형식 자체에 index에 정보가 있어 성능이 좋다.
- 인덱스 때문에 스키마의 순서가 중요하다.
